"""
TiDB IMPORT large dataset test suite.

This test file contains comprehensive tests for TiDB IMPORT functionality using
large datasets generated by the create_import.py program. It tests performance,
scalability, and various import scenarios with realistic data volumes.
"""

import os
import tempfile
import subprocess
import time
from src.common.test_rig_python import PyStateHandler, PyStateContext, PyState


class LargeDatasetImportHandler(PyStateHandler):
    """Test IMPORT INTO with large dataset (100k rows)."""
    
    def enter(self, context: PyStateContext) -> str:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS large_import_test")
            context.connection.execute_query("""
                CREATE TABLE large_import_test (
                    id INT PRIMARY KEY,
                    name VARCHAR(100),
                    age INT
                )
            """)
        return PyState.connecting()

    def execute(self, context: PyStateContext) -> str:
        if context.connection:
            # Generate test data using create_import.py
            temp_file = "temp_large_simple.csv"
            try:
                # Generate 100k rows of simple data
                cmd = ["python", "create_import.py", "--rows", "10000000", "--simple", "--output", temp_file]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
                
                if result.returncode != 0:
                    return PyState.error(f"Failed to generate test data: {result.stderr}")
                
                print(f"Generated test data: {temp_file}")
                
                # Execute IMPORT INTO with large dataset
                start_time = time.time()
                context.connection.execute_query(f"""
                    IMPORT INTO large_import_test
                    FROM '{temp_file}'
                    FORMAT CSV
                    FIELDS TERMINATED BY ','
                    LINES TERMINATED BY '\n'
                """)
                import_time = time.time() - start_time
                
                # Verify the data was imported
                result = context.connection.execute_query("SELECT COUNT(*) as count FROM large_import_test")
                if result and result[0].get('count', 0) == 10000000:
                    print(f"✅ Large import successful: 10,000,000 rows in {import_time:.2f} seconds")
                    return PyState.completed()
                else:
                    return PyState.error(f"Data count verification failed. Expected 100000, got {result[0].get('count', 0) if result else 0}")
            finally:
                # Clean up temp file
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
        return PyState.completed()

    def exit(self, context: PyStateContext) -> None:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS large_import_test")


class ComplexDatasetImportHandler(PyStateHandler):
    """Test IMPORT INTO with complex dataset (10k rows with multiple columns)."""
    
    def enter(self, context: PyStateContext) -> str:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS complex_import_test")
            context.connection.execute_query("""
                CREATE TABLE complex_import_test (
                    id INT PRIMARY KEY,
                    name VARCHAR(100),
                    email VARCHAR(200),
                    phone VARCHAR(20),
                    city VARCHAR(50),
                    department VARCHAR(50),
                    job_title VARCHAR(100),
                    salary INT,
                    performance_score DECIMAL(5,2),
                    hire_date DATE,
                    is_active BOOLEAN,
                    notes TEXT,
                    years_experience INT,
                    projects_completed INT
                )
            """)
        return PyState.connecting()

    def execute(self, context: PyStateContext) -> str:
        if context.connection:
            # Generate test data using create_import.py
            temp_file = "temp_complex.csv"
            try:
                # Generate 10k rows of complex data
                cmd = ["python", "create_import.py", "--rows", "1000000", "--output", temp_file]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
                
                if result.returncode != 0:
                    return PyState.error(f"Failed to generate test data: {result.stderr}")
                
                print(f"Generated complex test data: {temp_file}")
                
                # Execute IMPORT INTO with complex dataset
                start_time = time.time()
                context.connection.execute_query(f"""
                    IMPORT INTO complex_import_test
                    FROM '{temp_file}'
                    FORMAT CSV
                    FIELDS TERMINATED BY ','
                    OPTIONALLY ENCLOSED BY '"'
                    LINES TERMINATED BY '\n'
                    IGNORE 1 LINES
                """)
                import_time = time.time() - start_time
                
                # Verify the data was imported
                result = context.connection.execute_query("SELECT COUNT(*) as count FROM complex_import_test")
                if result and result[0].get('count', 0) == 1000000:
                    print(f"✅ Complex import successful: 1,000,000 rows in {import_time:.2f} seconds")
                    return PyState.completed()
                else:
                    return PyState.error(f"Data count verification failed. Expected 10000, got {result[0].get('count', 0) if result else 0}")
            finally:
                # Clean up temp file
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
        return PyState.completed()

    def exit(self, context: PyStateContext) -> None:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS complex_import_test")


class TSVImportHandler(PyStateHandler):
    """Test IMPORT INTO with TSV format large dataset."""
    
    def enter(self, context: PyStateContext) -> str:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS tsv_import_test")
            context.connection.execute_query("""
                CREATE TABLE tsv_import_test (
                    id INT PRIMARY KEY,
                    name VARCHAR(100),
                    email VARCHAR(200),
                    phone VARCHAR(20),
                    city VARCHAR(50),
                    department VARCHAR(50),
                    job_title VARCHAR(100),
                    salary INT,
                    performance_score DECIMAL(5,2),
                    hire_date DATE,
                    is_active BOOLEAN,
                    notes TEXT,
                    years_experience INT,
                    projects_completed INT
                )
            """)
        return PyState.connecting()

    def execute(self, context: PyStateContext) -> str:
        if context.connection:
            # Generate test data using create_import.py
            temp_file = "temp_tsv.tsv"
            try:
                # Generate 5k rows of TSV data
                cmd = ["python", "create_import.py", "--rows", "5000000", "--format", "tsv", "--output", temp_file]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
                
                if result.returncode != 0:
                    return PyState.error(f"Failed to generate test data: {result.stderr}")
                
                print(f"Generated TSV test data: {temp_file}")
                
                # Execute IMPORT INTO with TSV dataset
                start_time = time.time()
                context.connection.execute_query(f"""
                    IMPORT INTO tsv_import_test
                    FROM '{temp_file}'
                    FORMAT TSV
                    FIELDS TERMINATED BY '\t'
                    LINES TERMINATED BY '\n'
                    IGNORE 1 LINES
                """)
                import_time = time.time() - start_time
                
                # Verify the data was imported
                result = context.connection.execute_query("SELECT COUNT(*) as count FROM tsv_import_test")
                if result and result[0].get('count', 0) == 5000000:
                    print(f"✅ TSV import successful: 5,000,000 rows in {import_time:.2f} seconds")
                    return PyState.completed()
                else:
                    return PyState.error(f"Data count verification failed. Expected 5000, got {result[0].get('count', 0) if result else 0}")
            finally:
                # Clean up temp file
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
        return PyState.completed()

    def exit(self, context: PyStateContext) -> None:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS tsv_import_test")


class PartitionedLargeImportHandler(PyStateHandler):
    """Test IMPORT INTO with large dataset into partitioned table."""
    
    def enter(self, context: PyStateContext) -> str:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS partitioned_large_test")
            context.connection.execute_query("""
                CREATE TABLE partitioned_large_test (
                    id INT PRIMARY KEY,
                    name VARCHAR(100),
                    age INT,
                    salary INT
                ) PARTITION BY RANGE (age) (
                    PARTITION p0 VALUES LESS THAN (25),
                    PARTITION p1 VALUES LESS THAN (35),
                    PARTITION p2 VALUES LESS THAN (45),
                    PARTITION p3 VALUES LESS THAN (55),
                    PARTITION p4 VALUES LESS THAN MAXVALUE
                )
            """)
        return PyState.connecting()

    def execute(self, context: PyStateContext) -> str:
        if context.connection:
            # Generate test data using create_import.py
            temp_file = "temp_partitioned.csv"
            try:
                # Generate 50k rows of data
                cmd = ["python", "create_import.py", "--rows", "5000000", "--simple", "--output", temp_file]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
                
                if result.returncode != 0:
                    return PyState.error(f"Failed to generate test data: {result.stderr}")
                
                print(f"Generated partitioned test data: {temp_file}")
                
                # Execute IMPORT INTO with partitioned table
                start_time = time.time()
                context.connection.execute_query(f"""
                    IMPORT INTO partitioned_large_test (id, name, age, salary)
                    FROM '{temp_file}'
                    FORMAT CSV
                    FIELDS TERMINATED BY ','
                    LINES TERMINATED BY '\n'
                    (id, name, age, salary)
                """)
                import_time = time.time() - start_time
                
                # Verify the data was imported
                result = context.connection.execute_query("SELECT COUNT(*) as count FROM partitioned_large_test")
                if result and result[0].get('count', 0) == 5000000:
                    print(f"✅ Partitioned import successful: 5,000,000 rows in {import_time:.2f} seconds")
                    return PyState.completed()
                else:
                    return PyState.error(f"Data count verification failed. Expected 50000, got {result[0].get('count', 0) if result else 0}")
            finally:
                # Clean up temp file
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
        return PyState.completed()

    def exit(self, context: PyStateContext) -> None:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS partitioned_large_test")


class DuplicateKeyLargeImportHandler(PyStateHandler):
    """Test IMPORT INTO with large dataset and duplicate key handling."""
    
    def enter(self, context: PyStateContext) -> str:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS duplicate_large_test")
            context.connection.execute_query("""
                CREATE TABLE duplicate_large_test (
                    id INT PRIMARY KEY,
                    name VARCHAR(100),
                    age INT
                )
            """)
            # Insert initial data
            context.connection.execute_query("INSERT INTO duplicate_large_test VALUES (1, 'alice', 25), (2, 'bob', 30)")
        return PyState.connecting()

    def execute(self, context: PyStateContext) -> str:
        if context.connection:
            # Generate test data using create_import.py
            temp_file = "temp_duplicate.csv"
            try:
                # Generate 1k rows of data (smaller for duplicate testing)
                cmd = ["python", "create_import.py", "--rows", "1000000", "--simple", "--output", temp_file]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
                
                if result.returncode != 0:
                    return PyState.error(f"Failed to generate test data: {result.stderr}")
                
                print(f"Generated duplicate test data: {temp_file}")
                
                # Execute IMPORT INTO with duplicate key handling
                start_time = time.time()
                context.connection.execute_query(f"""
                    IMPORT INTO duplicate_large_test
                    FROM '{temp_file}'
                    FORMAT CSV
                    FIELDS TERMINATED BY ','
                    LINES TERMINATED BY '\n'
                    ON DUPLICATE KEY UPDATE name = VALUES(name), age = VALUES(age)
                """)
                import_time = time.time() - start_time
                
                # Verify the data was imported
                result = context.connection.execute_query("SELECT COUNT(*) as count FROM duplicate_large_test")
                if result and result[0].get('count', 0) >= 1000000:
                    print(f"✅ Duplicate key import successful: {result[0].get('count', 0)} rows in {import_time:.2f} seconds")
                    return PyState.completed()
                else:
                    return PyState.error(f"Data count verification failed. Expected >=1000, got {result[0].get('count', 0) if result else 0}")
            finally:
                # Clean up temp file
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
        return PyState.completed()

    def exit(self, context: PyStateContext) -> None:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS duplicate_large_test")


class PerformanceImportHandler(PyStateHandler):
    """Test IMPORT INTO performance with very large dataset."""
    
    def enter(self, context: PyStateContext) -> str:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS performance_test")
            context.connection.execute_query("""
                CREATE TABLE performance_test (
                    id INT PRIMARY KEY,
                    name VARCHAR(100),
                    age INT,
                    email VARCHAR(200),
                    city VARCHAR(50),
                    department VARCHAR(50),
                    salary INT
                )
            """)
        return PyState.connecting()

    def execute(self, context: PyStateContext) -> str:
        if context.connection:
            # Generate test data using create_import.py
            temp_file = "temp_performance.csv"
            try:
                # Generate 200k rows for performance testing
                cmd = ["python", "create_import.py", "--rows", "20000000", "--output", temp_file]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
                
                if result.returncode != 0:
                    return PyState.error(f"Failed to generate test data: {result.stderr}")
                
                print(f"Generated performance test data: {temp_file}")
                
                # Execute IMPORT INTO with performance monitoring
                start_time = time.time()
                context.connection.execute_query(f"""
                    IMPORT INTO performance_test (id, name, age, email, city, department, salary)
                    FROM '{temp_file}'
                    FORMAT CSV
                    FIELDS TERMINATED BY ','
                    OPTIONALLY ENCLOSED BY '"'
                    LINES TERMINATED BY '\n'
                    IGNORE 1 LINES
                """)
                import_time = time.time() - start_time
                
                # Verify the data was imported
                result = context.connection.execute_query("SELECT COUNT(*) as count FROM performance_test")
                if result and result[0].get('count', 0) == 20000000:
                    rows_per_sec = 20000000 / import_time if import_time > 0 else 0
                    print(f"✅ Performance import successful: 20,000,000 rows in {import_time:.2f} seconds ({rows_per_sec:.0f} rows/sec)")
                    return PyState.completed()
                else:
                    return PyState.error(f"Data count verification failed. Expected 200000, got {result[0].get('count', 0) if result else 0}")
            finally:
                # Clean up temp file
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
        return PyState.completed()

    def exit(self, context: PyStateContext) -> None:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS performance_test")


class ErrorHandlingLargeImportHandler(PyStateHandler):
    """Test IMPORT INTO error handling with large dataset containing errors."""
    
    def enter(self, context: PyStateContext) -> str:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS error_large_test")
            context.connection.execute_query("""
                CREATE TABLE error_large_test (
                    id INT PRIMARY KEY,
                    name VARCHAR(100),
                    age INT
                )
            """)
        return PyState.connecting()

    def execute(self, context: PyStateContext) -> str:
        if context.connection:
            # Generate test data using create_import.py
            temp_file = "temp_error.csv"
            try:
                # Generate 1k rows of data
                cmd = ["python", "create_import.py", "--rows", "1000000", "--simple", "--output", temp_file]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
                
                if result.returncode != 0:
                    return PyState.error(f"Failed to generate test data: {result.stderr}")
                
                # Add some invalid rows to the file
                with open(temp_file, 'a') as f:
                    f.write("invalid,data,here\n")
                    f.write("999999,Valid Name,25\n")
                    f.write("another,invalid,row\n")
                
                print(f"Generated error test data: {temp_file}")
                
                # Execute IMPORT INTO with error handling
                start_time = time.time()
                context.connection.execute_query(f"""
                    IMPORT INTO error_large_test
                    FROM '{temp_file}'
                    FORMAT CSV
                    FIELDS TERMINATED BY ','
                    LINES TERMINATED BY '\n'
                    IGNORE 1 LINES
                """)
                import_time = time.time() - start_time
                
                # Verify the data was imported (should handle errors gracefully)
                result = context.connection.execute_query("SELECT COUNT(*) as count FROM error_large_test")
                if result and result[0].get('count', 0) >= 1000000:
                    print(f"✅ Error handling import successful: {result[0].get('count', 0)} rows in {import_time:.2f} seconds")
                    return PyState.completed()
                else:
                    return PyState.error(f"Data count verification failed. Expected >=1000, got {result[0].get('count', 0) if result else 0}")
            finally:
                # Clean up temp file
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
        return PyState.completed()

    def exit(self, context: PyStateContext) -> None:
        if context.connection:
            context.connection.execute_query("DROP TABLE IF EXISTS error_large_test") 